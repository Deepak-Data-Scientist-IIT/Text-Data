{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embedding in Action(Word2Vec).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0kg1hJQkJfdz/f3g7X5EV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepak-Data-Scientist-IIT/Text-Data/blob/master/Word_Embedding_in_Action(Word2Vec).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYBesnDF0bE0",
        "colab_type": "text"
      },
      "source": [
        "# **1 Get Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4tEukG50e3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input Text\n",
        "text= ['Building some bots for Wikipedia.',\n",
        "       'Wikipedia is flooded with information.',\n",
        "       'There is an app for everything.']"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxej7VFg0iaq",
        "colab_type": "text"
      },
      "source": [
        "# **2 Text Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e_xL2eU0lxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning\n",
        "import re\n",
        "def clean(text):\n",
        "\n",
        "  # lower case\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove punctuations\n",
        "  text = re.sub('[^a-zA-Z]',\" \",text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H76hbqB0onA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call the clean function\n",
        "cleaned_text = []\n",
        "\n",
        "for i in text:\n",
        "  cleaned_text.append(clean(i))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-pBlxy-0rqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "696a0216-3ada-4bb2-db23-a1307dc9ff69"
      },
      "source": [
        "print(cleaned_text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['building some bots for wikipedia ', 'wikipedia is flooded with information ', 'there is an app for everything ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeyyZ05T0u_M",
        "colab_type": "text"
      },
      "source": [
        "# **3 Tokenzation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tGXv2mH0yXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f4c70120-3da5-4fa6-8db9-43f59e3d5e63"
      },
      "source": [
        "# Tokenize the text\n",
        "tokens = []\n",
        "\n",
        "for i in cleaned_text:\n",
        "  tokens.append(i.split())\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['building', 'some', 'bots', 'for', 'wikipedia'], ['wikipedia', 'is', 'flooded', 'with', 'information'], ['there', 'is', 'an', 'app', 'for', 'everything']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-8zc4B703iL",
        "colab_type": "text"
      },
      "source": [
        "# **4 Vocabulary Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ_-jTH405i9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2df6ec0-28ce-4cf4-fe30-5334dbb8a9d1"
      },
      "source": [
        "vocab = []\n",
        "\n",
        "for i in tokens:\n",
        "  for j in i:\n",
        "    if j not in vocab:\n",
        "      vocab.append(j)\n",
        "\n",
        "# remove duplicate tokens\n",
        "vocab = list(set(vocab))\n",
        "\n",
        "# sort tokens\n",
        "vocab.sort()\n",
        "\n",
        "print(vocab)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['an', 'app', 'bots', 'building', 'everything', 'flooded', 'for', 'information', 'is', 'some', 'there', 'wikipedia', 'with']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HW01UIK1ElQ",
        "colab_type": "text"
      },
      "source": [
        "# **5 Feature Representation (Word2Vec)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt8F2zkT2ci5",
        "colab_type": "text"
      },
      "source": [
        "# Download Google's pre-trained Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGqmoTo31OiG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "6c44386f-6b77-4b1b-a38b-a41538a373e6"
      },
      "source": [
        "# download and extract word2vec embeddings \n",
        "! wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "! gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-05 15:19:58--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.94.53\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.94.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  34.8MB/s    in 46s     \n",
            "\n",
            "2020-09-05 15:20:45 (34.3 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miGrowDP3UYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "579175cd-1c92-40c1-cb8a-ca230af61a0a"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# path of the downloaded model\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "# load into gensim\n",
        "w2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxnMLvBb3NvQ",
        "colab_type": "text"
      },
      "source": [
        "Once you have executed the above code, your word2vec embeddings are finally installed and loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USDYenJv3dv4",
        "colab_type": "text"
      },
      "source": [
        "Please note that the length of every vector of the pre-trained word2vec embeddings is 300."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRmpHWbw3Ap1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9fa784d7-5343-40d8-ca16-3aab0ec01cd1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# empty array of shape (no. of tokens X 300) to store word2vec features\n",
        "wordvec_array = np.zeros((len(vocab), 300))\n",
        "\n",
        "for i,j in enumerate(vocab):\n",
        "  wordvec_array[i,:] = w2vec.wv.word_vec(j)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6pprgZ732YY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "d4b24a4d-3665-455a-afc7-f2e89c03089c"
      },
      "source": [
        "wordvec_array"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.12597656,  0.19042969,  0.06982422, ...,  0.0612793 ,\n",
              "         0.17285156, -0.07861328],\n",
              "       [ 0.11572266, -0.29101562, -0.30664062, ..., -0.24609375,\n",
              "        -0.17773438,  0.16113281],\n",
              "       [ 0.41992188,  0.12011719, -0.06787109, ...,  0.00836182,\n",
              "        -0.25976562,  0.0279541 ],\n",
              "       ...,\n",
              "       [ 0.09423828, -0.02282715,  0.05224609, ..., -0.046875  ,\n",
              "         0.16113281, -0.19921875],\n",
              "       [ 0.21875   , -0.12207031, -0.00296021, ..., -0.35351562,\n",
              "        -0.25195312, -0.11621094],\n",
              "       [-0.02490234,  0.02197266, -0.03540039, ...,  0.01080322,\n",
              "        -0.01879883, -0.06884766]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}